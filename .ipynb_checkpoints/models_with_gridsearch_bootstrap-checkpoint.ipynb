{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import cross_validation\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    df = pd.read_csv('datasets.csv')\n",
    "    smells = ['OSE', 'BCE', 'PDE', 'SV', 'OS', 'SDS', 'RS', 'TFS']\n",
    "\n",
    "    m = Models()\n",
    "    for smell in smells:\n",
    "        print('### Predicting code smell \"{}\" ###'.format(smell))\n",
    "        m.predict(df, smell)\n",
    "\n",
    "class Models():\n",
    "\n",
    "    def predict(self, df, feature):\n",
    "        # train test split\n",
    "        X = df.iloc[:, 1:24]\n",
    "        y = df = df[[feature]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "        # Naive Bayes\n",
    "        print('-- Naive Bayes --')\n",
    "        clf = GaussianNB()\n",
    "        self.output_accuracy(X, y, X_train, y_train, X_test, y_test, clf)\n",
    "\n",
    "        # Random Forests\n",
    "        print('-- Random Forests --')\n",
    "        # Number of trees in random forest\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 10, stop = 50, num = 10)]\n",
    "        # Number of features to consider at every split\n",
    "        max_features = ['auto', 'sqrt']\n",
    "        # Maximum number of levels in tree\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        # Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [1, 2, 4]\n",
    "        # Method of selecting samples for training each tree\n",
    "        bootstrap = [True, False]\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=0)\n",
    "        parameters ={'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "        \n",
    "        clf = GridSearchCV(rf, parameters, cv = 10, n_jobs=4)\n",
    "        self.output_accuracy(X, y, X_train, y_train, X_test, y_test, clf)\n",
    "\n",
    "        # C4.5 (J48)\n",
    "#         print('-- C4.5 (implented as J48 in Weka) --')\n",
    "        decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "        parameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n",
    "        \n",
    "        clf = GridSearchCV(decision_tree, parameters, cv = 10, n_jobs=4)\n",
    "        self.output_accuracy(X, y, X_train, y_train, X_test, y_test, clf)\n",
    "\n",
    "        # Support Vector Machine using LIBSVM implementation with SMO\n",
    "        print('-- Support Vector Machine using LIBSVM implementation with SMO --')\n",
    "        svc = SVC(gamma='auto')\n",
    "        parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "        \n",
    "        clf = GridSearchCV(svc, parameters, cv = 10, n_jobs=4)\n",
    "        self.output_accuracy(X, y, X_train, y_train, X_test, y_test, clf)\n",
    "\n",
    "    def output_accuracy(self, X, y, X_train, y_train, X_test, y_test, clf):\n",
    "        # Does grid search over the parameter space\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # print outs detailed report on Grid Param search\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(clf.best_params_)\n",
    "        print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "        print()\n",
    "        \n",
    "        # Now test the best classifier on train and test set respectively        \n",
    "        train_acc = clf.score(X_train, y_train.values.ravel())\n",
    "        test_acc = clf.score(X_test, y_test.values.ravel())\n",
    "        print('Training accuray: {}'.format(train_acc))\n",
    "        print('Testing accuray: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
